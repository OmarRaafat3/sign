{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e2c17b-44fa-42ab-9871-7699f4425b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response\n",
    "from flask_cors import CORS\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import threading\n",
    "\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Load trained model\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Labels dictionary for predictions\n",
    "labels_dict = {i: chr(65 + i) for i in range(26)}\n",
    "\n",
    "# List to store detected letters and form the sentence\n",
    "sentence = []\n",
    "current_letter = \"?\"  # Track the current detected letter\n",
    "final_sentence = \"\"  # Store the printed sentence\n",
    "\n",
    "\n",
    "def generate_frames():\n",
    "    global current_letter, sentence, final_sentence  # Access global variables\n",
    "    cap = cv2.VideoCapture(0)  # Open camera\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to RGB for MediaPipe processing\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Default to \"?\" if no hand is detected\n",
    "        current_letter = \"?\"\n",
    "\n",
    "        # If hand landmarks are detected, predict the character\n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = results.multi_hand_landmarks[0]\n",
    "            data_aux = []\n",
    "\n",
    "            x_, y_ = [lm.x for lm in landmarks.landmark], [lm.y for lm in landmarks.landmark]\n",
    "            for lm in landmarks.landmark:\n",
    "                data_aux.append(lm.x - min(x_))\n",
    "                data_aux.append(lm.y - min(y_))\n",
    "\n",
    "            # Ensure proper data length for model input\n",
    "            data_aux = data_aux[:42] + [0] * max(0, 42 - len(data_aux))\n",
    "            prediction = model.predict([np.asarray(data_aux)])\n",
    "            current_letter = labels_dict.get(int(prediction[0]), \"?\")\n",
    "\n",
    "        # Add the current forming sentence, detected letter, and final sentence to the video frame\n",
    "        cv2.putText(frame, f\"Current Letter: {current_letter}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, \"Forming Sentence: \" + \"\".join(sentence), (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, \"Final Sentence: \" + final_sentence, (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Sign Language Detection\", frame)\n",
    "\n",
    "        # Listen for key presses\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('k'):  # Save the detected letter\n",
    "            if current_letter != \"?\":\n",
    "                sentence.append(current_letter)\n",
    "                print(f\"Saved Letter: {current_letter}\")\n",
    "        elif key == ord('s'):  # Add a space\n",
    "            sentence.append(\" \")\n",
    "            print(\"Added Space.\")\n",
    "        elif key == ord('d'):  # Delete the last letter (Backspace functionality)\n",
    "            if sentence:\n",
    "                deleted_letter = sentence.pop()\n",
    "                print(f\"Deleted Letter: {deleted_letter}\")\n",
    "            else:\n",
    "                print(\"No letter to delete.\")\n",
    "        elif key == ord('p'):  # Print the final sentence on screen and reset `sentence`\n",
    "            final_sentence = \"\".join(sentence)\n",
    "            print(f\"Final Sentence Printed: {final_sentence}\")\n",
    "            sentence = []  # Clear the forming sentence after printing\n",
    "        elif key == ord('q'):  # Quit the loop and stop the program\n",
    "            print(\"Exiting...\")\n",
    "            \n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "@app.route('/video_feed')\n",
    "\n",
    "def video_feed():\n",
    "    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "# Run Flask app in a separate thread\n",
    "def run_app():\n",
    "    app.run(debug=True, use_reloader=False)\n",
    "\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n",
    "\n",
    "# Run video capture and key detection in the main thread\n",
    "# generate_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022a04e-e139-4605-b05b-6351d174b6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5001\n",
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5001\n",
      "Press CTRL+C to quit\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from werkzeug.utils import secure_filename\n",
    "import nest_asyncio  # Required for Jupyter notebooks\n",
    "\n",
    "# Allow Flask to run smoothly inside Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Flask app and enable CORS\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"./sign_language_model.h5\")  # Check model path\n",
    "\n",
    "# Class labels for prediction\n",
    "class_labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "                'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "# Function to preprocess images for prediction\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
    "    img = cv2.resize(img, (64, 64))  # Resize as per model input shape\n",
    "    img = img / 255.0  # Normalize image (pixel values between 0 and 1)\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# API route to handle image uploads and predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_sign():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"No file part in the request\"}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "\n",
    "    if file.filename == '':\n",
    "        return jsonify({\"error\": \"No selected file\"}), 400\n",
    "\n",
    "    if file:\n",
    "        # Save the uploaded file temporarily\n",
    "        filename = secure_filename(file.filename)\n",
    "        file_path = os.path.join(\"temp\", filename)\n",
    "        os.makedirs(\"temp\", exist_ok=True)  # Ensure the temp folder exists\n",
    "        file.save(file_path)\n",
    "\n",
    "        # Preprocess the saved image and make a prediction\n",
    "        processed_img = preprocess_image(file_path)\n",
    "        prediction = model.predict(processed_img)\n",
    "        predicted_class = class_labels[np.argmax(prediction)]\n",
    "\n",
    "        # Clean up: remove the temporary image file\n",
    "        os.remove(file_path)\n",
    "\n",
    "        # Return the predicted class in the response\n",
    "        return jsonify({\"predicted_class\": predicted_class})\n",
    "\n",
    "    return jsonify({\"error\": \"File upload failed\"}), 500\n",
    "\n",
    "\n",
    "# Run Flask app with Jupyter compatibility\n",
    "app.run(debug=True, use_reloader=False, port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af5e1d-d6c1-43d4-a4cf-df875ebb3157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
